{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue May  7 14:29:49 2019\n",
    "\n",
    "@author: Nathan Tefft\n",
    "\n",
    "This script extracts data from the raw FARS data files to be used for replicating Levitt & Porter (2001). Selected variables are included, \n",
    "and the data definitions are harmonized across years. Accident, vehicle, and person dataframes are constructed and stored in csv files \n",
    "for later use in the replication.\n",
    "\"\"\"\n",
    "\n",
    "# This script has been validated for FARS datasets from 1982 to 2017\n",
    "\n",
    "earliestYear=1982\n",
    "latestYear = 2017\n",
    "\n",
    "# install and import necessary packages\n",
    "    # not all packages are standard installs\n",
    "    # for example, us is not included in anaconda\n",
    "    # you may need to install some of these packages in the command line\n",
    "\n",
    "import os, numpy, pandas, shutil, us, zipfile\n",
    "\n",
    "\"\"\"\n",
    "   USER-DEFINED ATTRIBUTES \n",
    "      \n",
    "1) The years over which FARS datasets are extracted and processed. \n",
    "    The default values are 1982 to 2017 \n",
    "        **FARS begins in 1975, but the first year of imputed BAC is 1982\n",
    "          If the user-provided first year is before 1982:\n",
    "              -First year is reset to 1982\n",
    "              -A warning message will appear to alert the user, but the script will not break.\n",
    "         **FARS is updated annually, but this script is only validated through 2017\n",
    "          If the user-provided last year is after 2017:\n",
    "              -Last year is reset to 2017\n",
    "              -A warning message will appear to alert the user, but the script will not break.     \n",
    "          \n",
    "    \n",
    "2) The working directory.\n",
    "    The user MUST set their own working directory before running the script. \n",
    "    We recommend the folder of the cloned GitHub repository.\n",
    "        For example, set the working directory to \"C:\\\\Users\\\\JoeEconomist\\\\GitHub\\\\lp\"\n",
    "    Data will then be placed into the subfolder .\\data\n",
    "\"\"\"\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\dunnr\\\\Documents\\\\GitHub\\\\lpdt\")\n",
    "\n",
    "# FARS data range\n",
    "firstYear = 1982\n",
    "lastYear = 1982\n",
    "\n",
    "\n",
    "if firstYear>lastYear:\n",
    "    print('User selected lastYear earlier than firstYear. firstYear has been set to ' + str(earliestYear) + ' and lastYear has been set to ' + str(latestYear) +'.')\n",
    "    firstYear = earliestYear\n",
    "    lastYear = latestYear\n",
    "if firstYear < earliestYear:\n",
    "    print('User selected firstYear prior to ' + str(earliestYear) + '. firstYear has been set to ' + str(earliestYear) +'.')\n",
    "    firstYear = earliestYear\n",
    "if lastYear > latestYear:\n",
    "    print('User selected lastYear after ' + str(latestYear) + '. lastYear has been set to ' + str(latestYear) +'.')\n",
    "    lastYear = latestYear\n",
    "\n",
    "# load US state abbreviations for later merge\n",
    "df_states = pandas.DataFrame.from_dict(us.states.mapping('fips', 'abbr'),orient='index',columns=['state_abbr'])\n",
    "df_states = df_states[df_states.index.notnull()]\n",
    "df_states.index = df_states.index.astype(int)\n",
    "\n",
    "# Initialize analytic dataframe for the accident, vehicle, and person datasets\n",
    "\n",
    "#fars_datasets = ['accident', 'vehicle', 'person', 'Miper']\n",
    "\n",
    "fars_datasets = ['accident', 'vehicle', 'person', 'Miper']\n",
    "dataset_ids = ['st_case','veh_no','per_no','per_no']\n",
    "\n",
    "df_list={}\n",
    "for dataset in fars_datasets:\n",
    "    df_list[dataset]=pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from 1982.\n",
      "accident(39092, 47)\n",
      "['year', 'st_case']\n",
      "vehicle(56455, 73)\n",
      "['year', 'st_case', 'veh_no']\n",
      "person(102120, 59)\n",
      "['year', 'st_case', 'veh_no', 'per_no']\n",
      "Miper(65070, 14)\n",
      "['year', 'st_case', 'veh_no', 'per_no']\n"
     ]
    }
   ],
   "source": [
    "# loop over years to be included in the replication analysis\n",
    "for yr in range(firstYear,lastYear+1): \n",
    "    print('Extracting data from ' + str(yr) + '.' )\n",
    "    \n",
    "    # extract accident, vehicle, person, and multiple imputation files\n",
    "    zipfile.ZipFile('data\\\\FARS' + str(yr) + '.zip', 'r').extractall(path='data\\\\extracted')\n",
    "    # UTF-8 encoding errors are ignored because they don't impact the relevant variables\n",
    "        \n",
    "    df_list_yr={}\n",
    "    index_list=['year']\n",
    "    for (dataset, id) in zip(fars_datasets,dataset_ids):\n",
    "        file = open('data\\\\extracted\\\\' + dataset + '.csv', errors='ignore')\n",
    "        df_list_yr[dataset]=pandas.read_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "        print(dataset + str(df_list_yr[dataset].shape))\n",
    "        df_list_yr[dataset].columns = df_list_yr[dataset].columns.str.lower() # make all columns lowercase\n",
    "        df_list_yr[dataset]['year']=numpy.full(len(df_list_yr[dataset].index), yr) # standardize the year variable to 4 digits\n",
    "        \n",
    "        if not dataset == 'Miper':\n",
    "            index_list.append(id)\n",
    "        print(index_list)\n",
    "        df_list_yr[dataset][index_list] = df_list_yr[dataset][index_list].astype('int') # set the indices as integers\n",
    "        df_list_yr[dataset].set_index(index_list, inplace=True) # set the multiindex\n",
    "        df_list_yr[dataset].index.set_names(index_list, inplace=True)  \n",
    "    '''    \n",
    "    # BAC MI dataset\n",
    "    dataset=\"Miper\"\n",
    "    file = open('data\\\\extracted\\\\' + dataset + '.csv', errors='ignore')\n",
    "    df_list_yr[dataset]=pandas.read_csv(file)\n",
    "    file.close()\n",
    "    \n",
    "    print(dataset + str(df_list_yr[dataset].shape))\n",
    "    df_list_yr[dataset].columns = df_list_yr[dataset].columns.str.lower() # make all columns lowercase\n",
    "    df_list_yr[dataset]['year']=numpy.full(len(df_list_yr[dataset].index), yr) # standardize the year variable to 4 digits\n",
    "    df_list_yr[dataset][index_list] = df_list_yr[dataset][index_list].astype('int') # set the indices as integers\n",
    "    df_list_yr[dataset].set_index(index_list, inplace=True) # set the multiindex\n",
    "    df_list_yr[dataset].index.set_names(index_list, inplace=True)\n",
    "    '''    \n",
    "    shutil.rmtree(path='data\\\\extracted') # clean up temporary extractions folder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    # manipulating accident data\n",
    "    df_list_yr['accident'].loc[df_list_yr['accident'].hour==99, 'hour'] = numpy.nan\n",
    "    df_list_yr['accident'].loc[df_list_yr['accident'].hour==24, 'hour'] = 0\n",
    "    df_list_yr['accident'].loc[df_list_yr['accident'].day_week==9, 'day_week'] = numpy.nan\n",
    "    df_list_yr['accident']['quarter'] = numpy.ceil(df_list_yr['accident']['month']/3) # create quarter variable\n",
    "    df_list_yr['accident'] = df_list_yr['accident'].merge(df_states,how='left',left_on='state',right_index=True) # merge in state abbreviations\n",
    "\n",
    "    # keep relevant accident variables and append to accident dataframe\n",
    "    df_list_yr['accident'] = df_list_yr['accident'][['state','state_abbr','quarter','day_week','hour','persons']]\n",
    "    print('Count of crashes: ' + str(len(df_list_yr['accident'])))\n",
    "    df_list['accident'] = df_list['accident'].append(df_list_yr['accident'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of vehicles: 56455\n"
     ]
    }
   ],
   "source": [
    "    # manipulating vehicle data\n",
    "    if yr <= 2008: \n",
    "        df_list_yr['vehicle']['occupants'] = df_list_yr['vehicle']['ocupants']\n",
    "    else:\n",
    "        df_list_yr['vehicle']['occupants'] = df_list_yr['vehicle']['numoccs']\n",
    "    if yr <= 2015:\n",
    "        df_list_yr['vehicle'].loc[df_list_yr['vehicle'].occupants>=99, 'occupants'] = numpy.nan\n",
    "    else:\n",
    "        df_list_yr['vehicle'].loc[df_list_yr['vehicle'].occupants>=97, 'occupants'] = numpy.nan\t\n",
    "    for vt in ['acc','sus','dwi','spd','oth']:\n",
    "        df_list_yr['vehicle'].loc[df_list_yr['vehicle']['prev_' + vt] > 97, 'prev_' + vt] = numpy.nan # previous violations\n",
    "\n",
    "    # keep relevant vehicle variables and append to vehicle dataframe\n",
    "    df_list_yr['vehicle'] = df_list_yr['vehicle'][['prev_acc','prev_sus','prev_dwi','prev_spd','prev_oth','dr_drink','occupants']]\n",
    "    print('Count of vehicles: ' + str(len(df_list_yr['vehicle'])))\n",
    "    df_list['vehicle'] = df_list['vehicle'].append(df_list_yr['vehicle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # manipulate person variables\n",
    "    \n",
    "    #standardize alcohol test result\n",
    "    if yr <= 1990: \n",
    "        df_list_yr['person']['alcohol_test_result'] = df_list_yr['person']['test_res']\n",
    "    else:\n",
    "        df_list_yr['person']['alcohol_test_result'] = df_list_yr['person']['alc_res']\n",
    "    \n",
    "    if yr >= 2015:\n",
    "        df_list_yr['person']['alcohol_test_result'] = df_list_yr['person']['alcohol_test_result']/10\n",
    "    \n",
    "    df_list_yr['person'].loc[df_list_yr['person'].alcohol_test_result>=95, 'alcohol_test_result'] = numpy.nan    \n",
    "    \n",
    "    \n",
    "    for vn in ['alc_det','atst_typ','race']: # create variables if necessary\n",
    "        if vn not in df_list_yr['person'].columns:\n",
    "            df_list_yr['person'][vn] = numpy.nan\n",
    "    \n",
    "    if yr <= 2008:\n",
    "        df_list_yr['person'].loc[df_list_yr['person'].age==99, 'age'] = numpy.nan # age\n",
    "    else:\n",
    "        df_list_yr['person'].loc[df_list_yr['person'].age>=998, 'age'] = numpy.nan # age\n",
    "    \n",
    "    df_list_yr['person']['age_lt15'] = df_list_yr['person']['age'] < 15 # less than 15 defined as child for our purposes\n",
    "    df_list_yr['person'].loc[df_list_yr['person'].sex.isin([8,9]), 'sex'] = numpy.nan # sex\n",
    "    df_list_yr['person'].loc[df_list_yr['person'].race==99, 'race'] = numpy.nan # race\n",
    "    df_list_yr['person'].loc[df_list_yr['person'].seat_pos>=98, 'seat_pos'] = numpy.nan # seat position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of persons: 102120\n"
     ]
    }
   ],
   "source": [
    "    # clean mulptiple imputation variables, e.g. harmonize names, ensure correct datatypes, and record missing variables \n",
    "    df_list_yr['Miper'] = df_list_yr['Miper'].rename(columns={'p1':'mibac1','p2':'mibac2','p3':'mibac3','p4':'mibac4','p5':'mibac5','p6':'mibac6','p7':'mibac7','p8':'mibac8','p9':'mibac9','p10':'mibac10'}) # rename bac columns    \n",
    "    df_list_yr['person'] = df_list_yr['person'].merge(df_list_yr['Miper'],how='left',on=['year','st_case','veh_no','per_no']) # merge multiply imputed bac values into person dataframe\n",
    "    \n",
    "    # keep relevant person variables and append to person dataframe\n",
    "    df_list_yr['person'] = df_list_yr['person'][['seat_pos','drinking','alc_det','atst_typ','alcohol_test_result','race','age','age_lt15','sex','mibac1','mibac2','mibac3','mibac4','mibac5','mibac6','mibac7','mibac8','mibac9','mibac10']]\n",
    "    print('Count of persons: ' + str(len(df_list_yr['person'])))\n",
    "    df_list['person'] = df_list['person'].append(df_list_yr['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describing dataframe accident\n",
      "              state       quarter      day_week          hour       persons\n",
      "count  39092.000000  39092.000000  39090.000000  38879.000000  39092.000000\n",
      "mean      28.140157      2.593344      4.240982     12.444224      2.612299\n",
      "std       15.973887      1.077992      2.151909      7.684351      1.748127\n",
      "min        1.000000      1.000000      1.000000      0.000000      1.000000\n",
      "25%       13.000000      2.000000      2.000000      5.000000      2.000000\n",
      "50%       28.000000      3.000000      4.000000     14.000000      2.000000\n",
      "75%       42.000000      4.000000      6.000000     19.000000      3.000000\n",
      "max       56.000000      4.000000      7.000000     23.000000     66.000000\n",
      "Describing dataframe vehicle\n",
      "           prev_acc      prev_sus      prev_dwi      prev_spd      prev_oth  \\\n",
      "count  53133.000000  53132.000000  53132.000000  53133.000000  53133.000000   \n",
      "mean       0.254663      0.132952      0.052962      0.533661      0.355147   \n",
      "std        0.586415      0.521919      0.278635      1.063138      0.823463   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
      "max        7.000000     12.000000      8.000000     20.000000     12.000000   \n",
      "\n",
      "           dr_drink     occupants  \n",
      "count  56455.000000  55208.000000  \n",
      "mean       0.305270      2.087306  \n",
      "std        0.460526      6.172339  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        1.000000      2.000000  \n",
      "max        1.000000     97.000000  \n",
      "Describing dataframe person\n",
      "           seat_pos       drinking  alc_det  atst_typ  alcohol_test_result  \\\n",
      "count  99863.000000  102120.000000      0.0       0.0         25730.000000   \n",
      "mean      12.529025       3.820456      NaN       NaN             9.874777   \n",
      "std        7.913891       3.954288      NaN       NaN            10.500493   \n",
      "min        0.000000       0.000000      NaN       NaN             0.000000   \n",
      "25%       11.000000       0.000000      NaN       NaN             0.000000   \n",
      "50%       11.000000       1.000000      NaN       NaN             8.000000   \n",
      "75%       13.000000       8.000000      NaN       NaN            18.000000   \n",
      "max       55.000000       9.000000      NaN       NaN            89.000000   \n",
      "\n",
      "       race           age            sex        mibac1        mibac2  \\\n",
      "count   0.0  99427.000000  100709.000000  65070.000000  65070.000000   \n",
      "mean    NaN     31.749122       1.298990      6.739219      6.753158   \n",
      "std     NaN     18.224785       0.457818      9.725400      9.652966   \n",
      "min     NaN      0.000000       1.000000      0.000000      0.000000   \n",
      "25%     NaN     19.000000       1.000000      0.000000      0.000000   \n",
      "50%     NaN     26.000000       1.000000      0.000000      0.000000   \n",
      "75%     NaN     41.000000       2.000000     14.000000     14.000000   \n",
      "max     NaN     97.000000       2.000000     75.000000     75.000000   \n",
      "\n",
      "             mibac3        mibac4        mibac5        mibac6        mibac7  \\\n",
      "count  65070.000000  65070.000000  65070.000000  65070.000000  65070.000000   \n",
      "mean       6.815568      6.721316      6.727970      6.752789      6.689565   \n",
      "std        9.699909      9.701803      9.666294      9.694161      9.666053   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       14.000000     14.000000     14.000000     14.000000     14.000000   \n",
      "max       75.000000     78.000000     75.000000     75.000000     75.000000   \n",
      "\n",
      "             mibac8        mibac9       mibac10  \n",
      "count  65070.000000  65070.000000  65070.000000  \n",
      "mean       6.810312      6.766621      6.751299  \n",
      "std        9.750112      9.689442      9.676747  \n",
      "min        0.000000      0.000000      0.000000  \n",
      "25%        0.000000      0.000000      0.000000  \n",
      "50%        0.000000      0.000000      0.000000  \n",
      "75%       14.000000     14.000000     14.000000  \n",
      "max       75.000000     77.000000     75.000000  \n"
     ]
    }
   ],
   "source": [
    "# summarize the constructed dataframes and save to csv files\n",
    "if not os.path.exists('replication\\\\data'):\n",
    "    os.makedirs('replication\\\\data')\n",
    "for dfn in fars_datasets:\n",
    "    print('Describing dataframe ' + dfn)\n",
    "    print(df_list[dfn].describe())\n",
    "    df_list[dfn].to_csv('replication\\\\data\\\\df_' + dfn + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from 1982.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-ef299957966b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdf_list_yr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfars_datasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data\\\\extracted\\\\'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdf_list_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_id' is not defined"
     ]
    }
   ],
   "source": [
    "# loop over years to be included in the replication analysis\n",
    "for yr in range(firstYear,lastYear+1): \n",
    "    print('Extracting data from ' + str(yr) + '.' )\n",
    "    \n",
    "    # extract accident, vehicle, person, and multiple imputation files\n",
    "    zipfile.ZipFile('data\\\\FARS' + str(yr) + '.zip', 'r').extractall(path='data\\\\extracted')\n",
    "    # UTF-8 encoding errors are ignored because they don't impact the relevant variables\n",
    "    #, 'vehicle', 'person', 'Miper'\n",
    "    \n",
    "    df_list_yr={}\n",
    "    for (dataset, id) in zip(fars_datasets,dataset_id):\n",
    "        file = open('data\\\\extracted\\\\' + dataset + '.csv', errors='ignore')\n",
    "        df_list_yr[dataset]=pandas.read_csv(file)\n",
    "        file.close()\n",
    "        \n",
    "        print(dataset + str(df_list_yr[dataset].shape))\n",
    "        df_list_yr[dataset].columns = df_list_yr[dataset].columns.str.lower() \n",
    "    \n",
    "    shutil.rmtree(path='data\\\\extracted') # clean up temporary extractions folde\n",
    "            \n",
    "    \n",
    "    df_list_yr['accident']['st_case'] = df_list_yr['accident']['st_case'].astype('int')\n",
    "    df_list_yr['accident'].set_index([numpy.full(len(df_list_yr['accident'].index), yr),'st_case'],inplace=True) # set the multiindex as year and st_case\n",
    "    df_list_yr['accident'].index.set_names(['year','st_case'], inplace=True) # set the multiindex names\n",
    "    df_list_yr['accident'].loc[df_list_yr['accident'].hour==99, 'hour'] = numpy.nan\n",
    "    df_list_yr['accident'].loc[df_list_yr['accident'].hour==24, 'hour'] = 0\n",
    "    df_list_yr['accident'].loc[df_list_yr['accident'].day_week==9, 'day_week'] = numpy.nan\n",
    "    df_list_yr['accident']['quarter'] = numpy.ceil(df_list_yr['accident']['month']/3) # create quarter variable\n",
    "    df_list_yr['accident'] = df_list_yr['accident'].merge(df_states,how='left',left_on='state',right_index=True) # merge in state abbreviations\n",
    "\n",
    "    # keep relevant accident variables and append to accident dataframe\n",
    "    df_list_yr['accident'] = df_list_yr['accident'][['state','state_abbr','quarter','day_week','hour','persons']].copy()\n",
    "    print('Count of crashes: ' + str(len(df_list_yr['accident'])))\n",
    "    df_list['accident'] = df_list['accident'].append(df_list_yr['accident'])\n",
    "    print('Count of crashes: ' + str(len(df_list['accident'])))\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from 1982.\n",
      "accident(39092, 47)\n",
      "['st_case']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all arrays must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-cd97351df50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdf_list_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_list_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#df_list_year[dataset].set_index(yr,index_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdf_list_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_list_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set the multiindex as year and st_case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mdf_list_yr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set the multiindex names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mset_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   4182\u001b[0m             \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4184\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index_from_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverify_integrity\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mensure_index_from_sequences\u001b[1;34m(sequences, names)\u001b[0m\n\u001b[0;32m   5313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5314\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5315\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py\u001b[0m in \u001b[0;36mfrom_arrays\u001b[1;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all arrays must be same length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_factorize_from_iterables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all arrays must be same length"
     ]
    }
   ],
   "source": [
    "# loop over years to be included in the replication analysis\n",
    "for yr in range(firstYear,lastYear+1): \n",
    "    print('Extracting data from ' + str(yr) + '.' )\n",
    "    \n",
    "    # extract accident, vehicle, person, and multiple imputation files\n",
    "    zipfile.ZipFile('data\\\\FARS' + str(yr) + '.zip', 'r').extractall(path='data\\\\extracted')\n",
    "    # UTF-8 encoding errors are ignored because they don't impact the relevant variables\n",
    "    #, 'vehicle', 'person', 'Miper'\n",
    "    \n",
    "    df_list_yr={}\n",
    "    index_list=[]\n",
    "    for (dataset, id) in zip(fars_datasets,dataset_ids):\n",
    "        file = open('data\\\\extracted\\\\' + dataset + '.csv', errors='ignore')\n",
    "        df_list_yr[dataset]=pandas.read_csv(file)\n",
    "        file.close()\n",
    "        #print(dataset + str(df_list_yr[dataset].shape))\n",
    "        #df_list_yr[dataset].columns = df_list_yr[dataset].columns.str.lower() \n",
    "    \n",
    "        print(dataset + str(df_list_yr[dataset].shape))\n",
    "        df_list_yr[dataset].columns = df_list_yr[dataset].columns.str.lower()\n",
    "        df_list_yr[dataset]['year']=numpy.full(len(df_list_yr[dataset].index), yr)\n",
    "        index_list.append(id)\n",
    "        print(index_list)\n",
    "        df_list_yr[dataset][index_list] = df_list_yr[dataset][index_list].astype('int')\n",
    "        #df_list_year[dataset].set_index(yr,index_list)\n",
    "        #df_list_yr[dataset].set_index([numpy.full(len(df_list_yr[dataset].index), yr), index_list],inplace=True) # set the multiindex as year and st_case\n",
    "        #df_list_yr[dataset].index.set_names(['year',index_list], inplace=True) # set the multiindex names\n",
    "    \n",
    "    shutil.rmtree(path='data\\\\extracted') # clean up temporary extractions folder\n",
    "    \n",
    "    # Accident dataset manipulation\n",
    "    \n",
    "    df_accident_yr = df_list_yr['accident'][['year','st_case','state', 'day_week','hour','month','persons']].copy()\n",
    "    df_accident_yr.loc[df_accident_yr.hour==99, 'hour'] = numpy.nan\n",
    "    df_accident_yr.loc[df_accident_yr.hour==24, 'hour'] = 0\n",
    "    df_accident_yr.loc[df_accident_yr.day_week==9, 'day_week'] = numpy.nan\n",
    "    df_accident_yr['quarter'] = numpy.ceil(df_accident_yr['month']/3) # create quarter variable\n",
    "    df_accident_yr = df_accident_yr.merge(df_states,how='left',left_on='state',right_index=True) # merge in state abbreviations\n",
    "\n",
    "    print('Count of crashes: ' + str(len(df_accident_yr)))\n",
    "    \n",
    "    df_list['accident'] = df_list['accident'].append(df_accident_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
